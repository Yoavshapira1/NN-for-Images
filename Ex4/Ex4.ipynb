{"cells":[{"cell_type":"markdown","metadata":{"id":"4XP4yTgN_g8s"},"source":["### **Imports**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xfMNvDuPyJ_B"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","import torchvision.transforms as transforms\n","import torchvision.models as models\n","import copy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tCh5UZcfyQCw"},"outputs":[],"source":["# Set GPU as default if available\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","torch.set_default_device(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x3WrM9q4xtAX"},"outputs":[],"source":["# Connect to Google Drive\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","root_dir = '/content/gdrive/MyDrive/NN for Images/Ex4'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gpFnlqA9x9rT"},"outputs":[],"source":["# Reproducability\n","import random\n","manualSeed = 999\n","random.seed(manualSeed)\n","torch.manual_seed(manualSeed)"]},{"cell_type":"markdown","metadata":{"id":"FFa00ExQaDJu"},"source":["W & B import"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lTGtZNNQaFmT"},"outputs":[],"source":["# API key: 62bc107823f2e5d51fd5f21b1081d81dd76a07db\n","!pip install wandb -qU\n","import wandb\n","wandb.login(key=\"62bc107823f2e5d51fd5f21b1081d81dd76a07db\")"]},{"cell_type":"markdown","metadata":{"id":"_a9yFt7__ZNn"},"source":["# **Some Constants, Operators and Functions**"]},{"cell_type":"markdown","metadata":{"id":"N06oEY06DUqp"},"source":["### Constants"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SgMob3Hwyj-U"},"outputs":[],"source":["# use small size if no GPU to save time\n","imsize = 512 if torch.cuda.is_available() else 128\n","\n","# Import the VGG net\n","cnn = models.vgg19(pretrained=True).features.eval()\n","\n","# Mean of the data VGG was pre-trained on\n","cnn_normalization_mean = torch.tensor([0.485, 0.456, 0.406])\n","\n","# STD of the data VGG was pre-trained on\n","cnn_normalization_std = torch.tensor([0.229, 0.224, 0.225])\n","\n","# List of layers to compute the Content-loss on\n","# taken from: ['conv_x' 'relu_x', 'pool_x', 'bn_x']\n","content_layers_default = ['conv_4']\n","\n","# List of layers to compute the Style-loss on\n","# taken from: ['conv_x' 'relu_x', 'pool_x', 'bn_x']\n","style_layers_default = ['conv_1', 'conv_2', 'conv_3', 'conv_4', 'conv_5']"]},{"cell_type":"markdown","metadata":{"id":"UDWldhsrDGwA"},"source":["### Image Utilities"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cuff6_VTDG5i"},"outputs":[],"source":["# Utility for the image loader\n","loader = transforms.Compose([\n","    transforms.Resize(imsize),\n","    transforms.ToTensor()])\n","\n","# Unloads a tensor into a PIL image\n","unloader = transforms.ToPILImage()\n","\n","# Load an image into a tensor\n","def image_loader(im_path):\n","    image = Image.open(im_path)\n","    image = loader(image).unsqueeze(0)\n","    return image.to(device, torch.float)\n","\n","# Plot an image from a tensor\n","def imshow(tensor, title=None):\n","    image = tensor.cpu().clone()\n","    image = image.squeeze(0)\n","    image = unloader(image)\n","    plt.imshow(image)\n","    if title is not None:\n","        plt.title(title)\n","\n","def show_and_save_image(plt, img, title, path):\n","    plt.figure()\n","    imshow(img, title=title)\n","    plt.axis('off')\n","    plt.savefig(path + r\"/{}.jpg\".format(title))\n","\n","# crop two images to fit their dimensions\n","def trim_images_to_fit_sizes(im1, im2):\n","    # Get the dimensions of the input images\n","    im1_dims = im1.shape\n","    im2_dims = im2.shape\n","\n","    # Calculate the target dimensions\n","    target_dims = (min(im1_dims[2], im2_dims[2]), min(im1_dims[3], im2_dims[3]))\n","\n","    # Calculate the starting positions for trimming\n","    start_pos_im1 = ((im1_dims[2] - target_dims[0]) // 2, (im1_dims[3] - target_dims[1]) // 2)\n","    start_pos_im2 = ((im2_dims[2] - target_dims[0]) // 2, (im2_dims[3] - target_dims[1]) // 2)\n","\n","    # Crop the images to the target dimensions\n","    cropped_im1 = im1[:, :, start_pos_im1[0]:start_pos_im1[0]+target_dims[0], start_pos_im1[1]:start_pos_im1[1]+target_dims[1]]\n","    cropped_im2 = im2[:, :, start_pos_im2[0]:start_pos_im2[0]+target_dims[0], start_pos_im2[1]:start_pos_im2[1]+target_dims[1]]\n","\n","    return cropped_im1, cropped_im2\n","\n","# create an input image: either a copy of content_img or noise of same size\n","def get_input_image(content_img, noise=False):\n","    if noise:\n","      return torch.randn(content_img.data.size())\n","    else:\n","      return content_img.clone()"]},{"cell_type":"markdown","metadata":{"id":"7bqF7MHpC8qr"},"source":["### Gram Matrix & Mean Var Calculators\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kyhX9hdFDCFI"},"outputs":[],"source":["def gram_matrix(input):\n","    a, b, c, d = input.size()  # a=batch size(=1)\n","    # b=number of feature maps\n","    # (c,d)=dimensions of a f. map (N=c*d)\n","    features = input.view(a * b, c * d)  # resize F_XL into \\hat F_XL\n","    G = torch.mm(features, features.t())  # compute the gram product\n","    # we 'normalize' the values of the gram matrix\n","    # by dividing by the number of element in each feature maps.\n","    return G.div(a * b * c * d)"]},{"cell_type":"code","source":["def meanVar(input):\n","    a, b, c, d = input.size()  # a=batch size(=1)\n","    # b=number of feature maps\n","    # (c,d)=dimensions of a f. map (N=c*d)\n","    features = input.view(a * b, c * d)\n","    mean = torch.mean(features,1)\n","    var = torch.var(features,1)\n","    meanVar = torch.cat((mean,var))\n","    #todo need to normalize meanVar?\n","    return meanVar"],"metadata":{"id":"_l3qfeb_1Xe8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dYm-AJP8B7g1"},"source":["###Some Sub-Modules\n","Those are sub-modules that will help to build the complete Style-Loss.\n","\n","***Content Loss***: Initialized with a target, and computes the MSE loss between the target and a given input.\n","\n","***Style Loss***: Initialized with a target, and computes the MSE between the Gram matrices of the target and a given input.\n","\n","In both modules, target and input should be a pure activation layers.\n","\n","***Normalization***: Initialized with a Mean and Std, and normalizes a given input by those values."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"28mn1pQrA28W"},"outputs":[],"source":["# Defining the Content-loss module\n","class ContentLoss(nn.Module):\n","    def __init__(self, target,):\n","        super(ContentLoss, self).__init__()\n","        self.target = target.detach()\n","\n","    def forward(self, input):\n","        self.loss = F.mse_loss(input, self.target)\n","        return input\n","\n","# Defining the Style-loss module based on Gram matrix\n","class StyleLoss(nn.Module):\n","    def __init__(self, target_feature):\n","        super(StyleLoss, self).__init__()\n","        self.target = gram_matrix(target_feature).detach()\n","\n","    def forward(self, input):\n","        G = gram_matrix(input)\n","        self.loss = F.mse_loss(G, self.target)\n","        return input\n","\n","# Defining the Style-loss module based on MeanVar matrix\n","class MeanVarLoss(nn.Module):\n","    def __init__(self, target_feature):\n","        super(MeanVarLoss, self).__init__()\n","        self.target = meanVar(target_feature).detach()\n","\n","    def forward(self, input):\n","        MV = meanVar(input)\n","        self.loss = F.mse_loss(MV, self.target)\n","        return input\n","\n","# Defining the normalization module\n","class Normalization(nn.Module):\n","    def __init__(self, mean, std):\n","        super(Normalization, self).__init__()\n","        self.mean = torch.tensor(mean).view(-1, 1, 1)\n","        self.std = torch.tensor(std).view(-1, 1, 1)\n","\n","    def forward(self, img):\n","        return (img - self.mean) / self.std\n","\n","# Initializes an instance of Normalization with the VGG mean and std\n","VggNormalization = Normalization(cnn_normalization_mean, cnn_normalization_std)"]},{"cell_type":"markdown","metadata":{"id":"L9WmokLtDqku"},"source":["### Style-Loss Module Definition\n","This function build a Style-Loss model.\n","\n","The Style-Loss model copies the architecture of a given net (VGG in our case) to some point, and computes the content-loss and style-loss of particular given layers. It recieves:\n","\n","***Content-layers*** List of `cnn`'s layers to computes their Content Loss with respect to `content_img`. Each such layer is a `ContentLoss` module, that it's target is the output of the current Style-Loss model (up to this layer), on `content_img`. Those layers are responsible to ***maintain the required content*** on the fly.\n","\n"," ***Style-layers*** List of `cnn`'s layers to computes their Style Loss with respect to `style_img`. Each such layer is a `StyleLoss`/`MeanVarLoss` module, that it's target is the output of the current Style-Loss model (up to this layer), on `style_img`, where the Gram matrices are computed in the module itself. Those layers are responsible to ***maintain the required style*** on the fly.\n","\n"," So, given a network *cnn*, list of *Content-layers* and *Style-layers*, the final architecture of the Style-Loss Module is as follows (Let M be the model):\n","\n"," 1. `Normalization` (default is the VggNormalization)\n"," 2. For **L** in *cnn [first layer : last given layer]*:\n","\n","  * `L`\n","  * if L is in Content-layers => add `ContentLoss(target=M(content_img))` layer\n","  * if L is in Style-layers => add `StyleLoss(target=M(style_img))` layer (or `MeanVarLoss`)\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6l3S8SfVDqrK"},"outputs":[],"source":["def get_style_model_and_losses(cnn, style_img, content_img,\n","                               content_layers=content_layers_default,\n","                               style_layers=style_layers_default,\n","                               normalization=VggNormalization,\n","                               use_mean_var=False):\n","\n","    content_losses, style_losses  = [], []\n","\n","    # Initializes Sequential model with normalization\n","    style_loss_model = nn.Sequential(normalization)\n","\n","    i = 0  # increment every time we see a conv\n","    for layer in cnn.children():\n","        if isinstance(layer, nn.Conv2d):\n","            i += 1\n","            name = 'conv_{}'.format(i)\n","        elif isinstance(layer, nn.ReLU):\n","            name = 'relu_{}'.format(i)\n","            layer = nn.ReLU(inplace=False)\n","        elif isinstance(layer, nn.MaxPool2d):\n","            name = 'pool_{}'.format(i)\n","        elif isinstance(layer, nn.BatchNorm2d):\n","            name = 'bn_{}'.format(i)\n","        else:\n","            raise RuntimeError('Unrecognized layer: {}'.format(layer.__class__.__name__))\n","\n","        # add the layer to the model\n","        style_loss_model.add_module(name, layer)\n","\n","        ## if layer is in Content-layers, add a content loss\n","        if name in content_layers:\n","            target = style_loss_model(content_img).detach()\n","            content_loss = ContentLoss(target)\n","            style_loss_model.add_module(\"content_loss_{}\".format(i), content_loss)\n","            content_losses.append(content_loss)\n","\n","        ## if layer is in Style-layers, add a style loss\n","        if name in style_layers:\n","            target_feature = style_loss_model(style_img).detach()\n","            style_loss = MeanVarLoss(target_feature) if use_mean_var else StyleLoss(target_feature)\n","            style_loss_model.add_module(\"style_loss_{}\".format(i), style_loss)\n","            style_losses.append(style_loss)\n","\n","    # now we trim off the layers after the last content and style losses\n","    for i in range(len(style_loss_model) - 1, -1, -1):\n","        if isinstance(style_loss_model[i], ContentLoss) or isinstance(style_loss_model[i], StyleLoss) or isinstance(style_loss_model[i], MeanVarLoss):\n","            break\n","\n","    style_loss_model = style_loss_model[:(i + 1)]\n","\n","    return style_loss_model, style_losses, content_losses"]},{"cell_type":"markdown","metadata":{"id":"XUEKE_dHb4uQ"},"source":["# **Style Transfer Basic Algorithm**\n","Arguments:\n","\n","\n","*   ***cnn***: A convolutional neural network to copy it's architecture. In our case is VGG-19.\n","*   ***content_img***: The image that we want to change it's style\n","* ***style_img***: The image contains the style we want to apply\n","* ***input_img***: The image we optimize, this will be the result image. Can either be a copy of *content_image* or noise\n","* ***content_layers***: The layers to perform ContentLoss on\n","* ***style_layers***: The layers to perform StyleLoss on\n","* ***normalization***: Normalization module to fit the data to the cnn\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TRVIKDa-b-S9","cellView":"form"},"outputs":[],"source":["#@title Original Algorithm\n","def run_style_transfer(cnn, content_img, style_img, input_img,\n","                       content_layers=content_layers_default,\n","                       style_layers=style_layers_default,\n","                       normalization=VggNormalization,\n","                       num_steps=300,\n","                       style_weight=1000000,\n","                       content_weight=1,\n","                       content_converg=False):\n","\n","    print('Building the style transfer Style-loss model..')\n","    model, style_losses, content_losses = get_style_model_and_losses(cnn,\n","                                          style_img, content_img, content_layers,\n","                                          style_layers, normalization)\n","\n","    # we want to optimize the input image and not the model's parameters\n","    input_img.requires_grad_(True)\n","    model.eval()\n","    model.requires_grad_(False)\n","\n","    optimizer = optim.LBFGS([input_img])\n","\n","    # this allows us to iterate until content loss converges\n","    condition = lambda x: x <= num_steps if not content_converg else True\n","\n","    print('Optimizing..')\n","    run = [0]\n","    last_two_content_losses = [0, 1000]\n","    while condition(run[0]):\n","\n","        def closure():\n","            # correct the values of updated input image\n","            with torch.no_grad():\n","                input_img.clamp_(0, 1)\n","\n","            optimizer.zero_grad()\n","            model(input_img)\n","            style_score = 0\n","            content_score = 0\n","\n","            for sl in style_losses:\n","                style_score += sl.loss\n","            for cl in content_losses:\n","                content_score += cl.loss\n","\n","            style_score *= style_weight\n","            content_score *= content_weight\n","            last_two_content_losses[0] = last_two_content_losses[1]\n","            last_two_content_losses[1] = content_score.item()\n","\n","            loss = style_score + content_score\n","            loss.backward()\n","\n","            run[0] += 1\n","            if run[0] % 50 == 0:\n","                print(\"run {}:\".format(run))\n","                print('Style Loss : {:4f} Content Loss: {:4f}'.format(\n","                    style_score.item(), content_score.item()))\n","                print()\n","\n","            return style_score + content_score\n","\n","        optimizer.step(closure)\n","\n","        # check if content loss has converged and stop if need to.\n","        # limited to 2000 runs\n","        diff = abs(last_two_content_losses[1] - last_two_content_losses[0])\n","        if diff < 0.0001 or run[0] > 2000:\n","          print(\"Convergence at {} with diff {}\".format(run[0], diff))\n","          break\n","\n","    # a last correction...\n","    with torch.no_grad():\n","        input_img.clamp_(0, 1)\n","\n","    return input_img"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dGR4YLNrykBd","cellView":"form"},"outputs":[],"source":["#@title A Simple Style Transfer Example\n","def style_transfer_example():\n","    # load a style reference and a content image\n","    style_img = image_loader(root_dir + r\"/Images/Style/picasso.jpg\")\n","    content_img = image_loader(root_dir + r\"/Images/Content/dancing.jpg\")\n","    content_img_name = \"dancing\"\n","    style_img_name = \"picasso\"\n","\n","    # fit their dimensions\n","    if style_img.size() != content_img.size():\n","      style_img, content_img = trim_images_to_fit_sizes(style_img, content_img)\n","\n","    # get input image: clone or noise\n","    input_img = get_input_image(content_img, noise=True)\n","\n","    imshow(content_img, \"Content\")\n","    imshow(style_img, \"Style\")\n","    imshow(input_img, \"Input\")\n","\n","    # run the style transfer algorithm\n","    output = run_style_transfer(cnn, content_img, style_img, input_img, content_weight=3, content_converg=True)\n","\n","    # plot the results\n","    show_and_save_image(plt, output, \"{} in {} style\".format(content_img_name,\n","                                      style_img_name), path=root_dir+r\"/Results\")"]},{"cell_type":"code","source":["style_transfer_example()"],"metadata":{"id":"E6Q9FjO8Lt-X"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pNOFpNM1Sovn"},"source":["# **Part 1**"]},{"cell_type":"markdown","metadata":{"id":"q41WnezuSr7_"},"source":["## **A. Feature inversion**\n","\n","# modify the algorithm to invert features from different stages of the VGG-19 network (which is the pretrained feature extraction network used by the algorithm). More specifically, given a target image, record its feature map  at the output of one of the layers of VGG-19. Next initialize an input image to noise, and optimize the input until its feature map for that layer closely approximates . Experiment with inversion of layers of different types and at different depths of VGG-19.\n","\n","**Implementation**: We build a Content-Loss model using the parameters:\n","\n","1. `content_layers` = [L], where L is a layer in the VGG-19. We investigate here convolutional and ReLU layers in different depths.\n","2. `style_layers` = []. This way, the model won't make any style comparisons.\n","3. `input_img` = Noise\n","\n","We used the default values of hyperparameters for the learning loop, and record the losses for all different layers. The results are in the PDF file."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rMGefYIwbhH4","cellView":"form"},"outputs":[],"source":["#@title Part1_A Algorithm (w/o style loss)\n","def Part1_A_algo(cnn, content_img, style_img, input_img,\n","                       content_layers=content_layers_default,\n","                       style_layers=style_layers_default,\n","                       normalization=VggNormalization,\n","                       num_steps=300,\n","                       style_weight=1000000,\n","                       content_weight=1):\n","\n","    print('Content layer {}'.format(content_layers[0]))\n","    model, style_losses, content_losses = get_style_model_and_losses(cnn,\n","                                      style_img, content_img, content_layers,\n","                                      style_layers, normalization)\n","\n","    # we want to optimize the input image and not the model's parameters\n","    input_img.requires_grad_(True)\n","    model.eval()\n","    model.requires_grad_(False)\n","\n","    optimizer = optim.LBFGS([input_img])\n","\n","    print('Optimizing..')\n","    run = [0]\n","    while run[0] <= num_steps:\n","\n","        def closure():\n","            # correct the values of updated input image\n","            with torch.no_grad():\n","                input_img.clamp_(0, 1)\n","\n","            optimizer.zero_grad()\n","            model(input_img)\n","            content_score = 0\n","\n","            for cl in content_losses:\n","                content_score += cl.loss\n","            content_score *= content_weight\n","\n","            loss = content_score\n","            loss.backward()\n","\n","            # Plot the loss\n","            metrics = {\"Content_loss\": content_score.item()}\n","            wandb.log(metrics)\n","\n","            run[0] += 1\n","            if run[0] % 50 == 0:\n","                print(\"run {}:\".format(run))\n","                print('Content Loss: {:4f}'.format(content_score.item()))\n","                print()\n","\n","            return content_score\n","\n","        optimizer.step(closure)\n","\n","    # a last correction...\n","    with torch.no_grad():\n","        input_img.clamp_(0, 1)\n","\n","    return input_img"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HLwMoTtwU2Tw","cellView":"form"},"outputs":[],"source":["#@title Question Implementation\n","\n","def Part1_A():\n","    # set path to save figures in. losses plots are saved to W&B\n","    part1_a_path = root_dir + r\"/Part1/A\"\n","\n","    # investigate all conv & relu layers of VGG-19\n","    content_layers = [\"conv_%d\" % d for d in range(1,14)] + [\"relu_%d\" % d for d in range(1,14)]\n","\n","    content_img = image_loader(root_dir + r\"/Images/Content/dancing.jpg\")\n","\n","    for layer in content_layers:\n","        input_img = get_input_image(content_img, noise=True)\n","        wandb.init(\n","            project=\"Ex4_Part1_A\",\n","            config={\n","                'name': layer,\n","                'layer': layer,\n","                'type': layer[:4],   # either 'conv' or 'relu'\n","                'depth': layer[-1] if layer[-2] == \"_\" else layer[-2:]\n","                })\n","        config = wandb.config\n","\n","        # run the modified algorithm, with no weight on the style loss\n","        # and much more weight on the content loss\n","        output = Part1_A_algo(cnn, content_img, None, input_img,\n","                              content_layers=[config.layer], style_layers = [],\n","                              style_weight=0, content_weight=1000000, num_steps=500)\n","        wandb.finish()\n","\n","        # save optimized image\n","        show_and_save_image(plt, output, layer, part1_a_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QwET_NNvg92O"},"outputs":[],"source":["Part1_A()"]},{"cell_type":"markdown","source":["## **B. Texture synthesis**\n","\n","# modify the algorithm to generate a texture inspired by the style input, and demonstrate how using Gram matrices from different network layers affects the characteristics of the generated texture.\n","\n","\n","**Implementation**: We build a Style-Loss model using the parameters:\n","\n","1. `content_layers` = [] This way, the model won't make any content comparisons.\n","2. `style_layers` = [L]. where L is a layer in the VGG-19. We investigate here the Gram matrices convolutional and ReLU layers in different depths.\n","3. `input_img` = Noise\n","\n","We used the default values of hyperparameters for the learning loop, and record the losses for all different layers. The results are in the PDF file."],"metadata":{"id":"78KJ-iLZsaQt"}},{"cell_type":"code","source":["#@title Part1_B Algorithm (w/o content loss)\n","def Part1_B_algo(cnn, content_img, style_img, input_img,\n","                       content_layers=content_layers_default,\n","                       style_layers=style_layers_default,\n","                       normalization=VggNormalization,\n","                       num_steps=300,\n","                       style_weight=1000000,\n","                       content_weight=1):\n","\n","    print('style layer {}'.format(style_layers[0]))\n","    model, style_losses, content_losses = get_style_model_and_losses(cnn,\n","                                      style_img, content_img, content_layers,\n","                                      style_layers, normalization)\n","\n","    # we want to optimize the input image and not the model's parameters\n","    input_img.requires_grad_(True)\n","    model.eval()\n","    model.requires_grad_(False)\n","\n","    optimizer = optim.LBFGS([input_img])\n","\n","    print('Optimizing..')\n","    run = [0]\n","    while run[0] <= num_steps:\n","\n","        def closure():\n","            # correct the values of updated input image\n","            with torch.no_grad():\n","                input_img.clamp_(0, 1)\n","\n","            optimizer.zero_grad()\n","            model(input_img)\n","            style_score = 0\n","\n","\n","            for sl in style_losses:\n","                style_score += sl.loss\n","\n","            style_score *= style_weight\n","\n","            loss = style_score\n","            loss.backward()\n","\n","            # Plot the loss\n","            metrics = {\"Style_loss\": style_score.item()}\n","            wandb.log(metrics)\n","\n","            run[0] += 1\n","            if run[0] % 50 == 0:\n","                print(\"run {}:\".format(run))\n","                print('Style Loss: {:4f}'.format(style_score.item()))\n","                print()\n","\n","            return style_score\n","\n","        optimizer.step(closure)\n","\n","    # a last correction...\n","    with torch.no_grad():\n","        input_img.clamp_(0, 1)\n","\n","    return input_img"],"metadata":{"id":"DqAr3gQi4wCZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Question Implementation\n","def Part1_B():\n","    # set path to save figures in. losses plots are saved to W&B\n","    part1_b_path = root_dir + r\"/Part1/B\"\n","\n","    # investigate all conv & relu layers of VGG-19\n","    #todo what layers to use?\n","    style_layers = [\"conv_%d\" % d for d in range(1,14)] + [\"relu_%d\" % d for d in range(1,14)]\n","\n","    style_img = image_loader(root_dir + r\"/Images/Style/picasso.jpg\")\n","\n","    for layer in style_layers:\n","        input_img = get_input_image(style_img, noise=True)\n","        wandb.init(\n","            project=\"Ex4_Part1_B\",\n","            config={\n","                'name': layer,\n","                'layer': layer,\n","                'type': layer[:4],   # either 'conv' or 'relu'\n","                'depth': layer[-1] if layer[-2] == \"_\" else layer[-2:]\n","                })\n","        config = wandb.config\n","\n","        # run the modified algorithm, with no weight on the style loss\n","        # and much more weight on the content loss\n","        output = Part1_B_algo(cnn, None, style_img, input_img,\n","                              content_layers=[], style_layers = [config.layer],\n","                              style_weight=1000000, content_weight=0, num_steps=500)\n","        wandb.finish()\n","\n","        # save optimized image\n","        show_and_save_image(plt, output, layer, part1_b_path)"],"metadata":{"id":"rRPjFrpn6j2-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["Part1_B()"],"metadata":{"id":"UwuhSAh07izw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **C. MeanVar style loss**"],"metadata":{"id":"v1XGfs4JToQF"}},{"cell_type":"markdown","source":["### Implement a “MeanVar” style loss that computes and compares the mean and the variance of each activation channel, in a given layer, instead of the Gram matrix of the layer. Compare the style transfer results obtained with this style loss to those obtained by the original implementation that uses the Gram matrices.\n","\n","\n","**Implementation**: We have implemeneted a style-transfer using several style-loss layers, and one content loss layer (those are the default values declared on top). The algorithm is the 'Original Algorithm' defined above, and for the Style-Loss model here, we defined it ot be based on MeanVar matrices style-comparisons rather then the original Gram matrices.\n","\n","The definition of the `MeanVarLoss` is located in section \"Sub Modules\"."],"metadata":{"id":"zwsFWPSdTcfR"}},{"cell_type":"code","source":["#@title Question Implementation\n","def Part1_C():\n","    # load a style reference and a content image\n","    style_img = image_loader(root_dir + r\"/Images/Style/picasso.jpg\")\n","    content_img = image_loader(root_dir + r\"/Images/Content/dancing.jpg\")\n","    content_img_name = \"dancing\"\n","    style_img_name = \"picasso\"\n","    # fit their dimensions\n","    if style_img.size() != content_img.size():\n","      style_img, content_img = trim_images_to_fit_sizes(style_img, content_img)\n","\n","    # get input image: clone or noise\n","    input_img = get_input_image(content_img, noise=False)\n","\n","    imshow(content_img, \"Content\")\n","    imshow(style_img, \"Style\")\n","    imshow(input_img, \"Input\")\n","\n","    # run the style transfer algorithm\n","    style_weight = 10000\n","    output = run_style_transfer(cnn, content_img, style_img, input_img,style_weight=style_weight,content_weight=3, content_converg=True, use_mean_var=True)\n","\n","    # plot the results\n","    show_and_save_image(plt, output, \"{} in {} style - meanVar Loss style_weight{}\".format(content_img_name,\n","                                      style_img_name,style_weight), path=root_dir+r\"/Results\")"],"metadata":{"id":"vX9SCTc6JG9L","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["Part1_C()"],"metadata":{"id":"-N5R7Sy9JhqV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 2**"],"metadata":{"id":"HiNfPjCY5fS1"}},{"cell_type":"markdown","source":["# Requirements and installations"],"metadata":{"id":"K_hKxZ5qM2-Y"}},{"cell_type":"code","source":["!pip install transformers\n","!pip install diffusers\n","\n","import itertools\n","from tqdm.auto import tqdm\n","from PIL import Image\n","import torch\n","from transformers import CLIPTextModel, CLIPTokenizer\n","from diffusers import AutoencoderKL, UNet2DConditionModel, PNDMScheduler,UniPCMultistepScheduler\n","\n","scheduler = UniPCMultistepScheduler.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"scheduler\")\n","vae = AutoencoderKL.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"vae\")\n","tokenizer = CLIPTokenizer.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"tokenizer\")\n","text_encoder = CLIPTextModel.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"text_encoder\")\n","unet = UNet2DConditionModel.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"unet\")"],"metadata":{"id":"Bj2m3UHB5eWm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device = \"cuda\"\n","vae.to(device).eval()\n","text_encoder.to(device).eval()\n","unet.to(device).eval()\n"],"metadata":{"id":"j-u_RdXI6RfZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["scheduler.set_timesteps(50,device=device)\n","scheduler.timesteps"],"metadata":{"id":"VzCnSvt4w3Qr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Text embeddings"],"metadata":{"id":"PrcfK6bvNSKa"}},{"cell_type":"code","source":["def text_embedding(prompt):\n","    generator = torch.manual_seed(217)  # Seed generator to create the inital latent noise\n","    batch_size = len(prompt)\n","    text_input = tokenizer(\n","        prompt, padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\"\n","    )\n","\n","    with torch.no_grad():\n","        text_embeddings = text_encoder(text_input.input_ids.to(device))[0]\n","\n","    max_length = text_input.input_ids.shape[-1]\n","    uncond_input = tokenizer([\"\"] * batch_size, padding=\"max_length\", max_length=max_length, return_tensors=\"pt\")\n","    uncond_embeddings = text_encoder(uncond_input.input_ids.to(device))[0]\n","\n","    text_embeddings = torch.cat([uncond_embeddings, text_embeddings])\n","\n","    return text_embeddings, batch_size"],"metadata":{"id":"jvhIf2s8MGuL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_latent_vec(batch_size):\n","    latents = torch.randn(\n","        (batch_size, unet.in_channels, height // 8, width // 8),\n","    )\n","    latents = latents.to(device)\n","    latents = latents * scheduler.init_noise_sigma\n","\n","    return latents"],"metadata":{"id":"ZYVlZhcHMRCw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_style_model_for_diffusion(style_img):\n","    model, style_losses, content_losses = get_style_model_and_losses(cnn,\n","                                          style_img, content_img, content_layers=[],\n","                                          use_mean_var=False)\n","    model.eval()\n","    return model,style_losses"],"metadata":{"id":"HAyMrHZEA2al"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def show_img(latents):\n","  scaled_latents = 1 / 0.18215 * latents\n","  with torch.no_grad():\n","    image = vae.decode(scaled_latents).sample\n","  # convert the image to a PIL.Image to see the generated image\n","  image = (image / 2 + 0.5).clamp(0, 1)\n","  image = image.detach().cpu().permute(0, 2, 3, 1).numpy()\n","  images = (image * 255).round().astype(\"uint8\")\n","  pil_images = [Image.fromarray(image) for image in images]\n","  return pil_images[0]"],"metadata":{"id":"o4pHCe8PXICo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def sl_grad(clean_latent_estimate,model,style_losses):\n","\n","    scaled_latent_estimate = 1 / 0.18215 * clean_latent_estimate\n","    vae.eval()\n","\n","    clean_img_pred = vae.decode(scaled_latent_estimate).sample / 2 + 0.5\n","\n","    model(clean_img_pred)\n","    style_score = 0\n","    for sl in style_losses:\n","        style_score += sl.loss\n","\n","    style_score.backward()\n","    return clean_img_pred"],"metadata":{"id":"zXotJiGyrBCc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def optimize_latents(latents, scheduler, num_inference_steps, style_img,\n","                     text_embeddings, guidance_scale, style_weight,use_StyleLoss=True):\n","\n","    scheduler.set_timesteps(num_inference_steps)\n","    if use_StyleLoss:\n","      model,style_losses = get_style_model_for_diffusion(style_img)\n","    for i,t in enumerate(tqdm(scheduler.timesteps)):\n","        latent_model_input = torch.cat([latents] * 2)\n","        latent_model_input = scheduler.scale_model_input(latent_model_input, timestep=t)\n","\n","        with torch.no_grad():\n","          noise_pred = unet(latent_model_input, t, encoder_hidden_states=text_embeddings).sample\n","        noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n","\n","        noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n","        if use_StyleLoss and i>20:\n","          z = noise_pred.clone().detach()\n","          z.requires_grad = True\n","          clean_latent_estimate = scheduler.convert_model_output(z, t, latents)\n","          clean_img = sl_grad(clean_latent_estimate,model,style_losses)\n","          with torch.no_grad():\n","            grad = z.grad / torch.mean(torch.abs(z.grad))\n","          if i > 50:\n","            grad *= 1.5\n","          sig_t = scheduler.sigma_t[t]\n","          noise_pred -= style_weight * sig_t * grad\n","\n","        latents = scheduler.step(noise_pred, t, latents).prev_sample\n","    torch.cuda.empty_cache()\n","\n","    return latents"],"metadata":{"id":"2RkJBz-KLNdU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["random.seed(212)\n","torch.manual_seed(212)\n","path = root_dir + r\"/Part2/A\"\n","# style_images = [\"monet_portrait.jpg\", \"vangoh_portrait.jpg\", \"picasso_portrait.jpg\", \"francis_portrait.jpg\"]\n","style_images = [\"picasso.jpg\"]\n","\n","text = [\"female actress on stage bow to audience\"]\n","height = 512  # default height of Stable Diffusion\n","width = 512  # default width of Stable Diffusion\n","torch.cuda.empty_cache()\n","content_img = None\n","num_inference_steps = 100  # Number of denoising steps\n","style_weights = [0.8]               # Style weight\n","guidance_scales = [7.5]  # Scale for classifier-free guidance (refer to text)\n","\n","for style_im in style_images:\n","  input_im = image_loader(root_dir + r\"/Images/Style/{}\".format(style_im))\n","  for style_s, guide_s in itertools.product(style_weights, guidance_scales):\n","    torch.cuda.empty_cache()\n","    text_embeddings, batch_size = text_embedding(text)\n","    latents = get_latent_vec(batch_size)\n","    latents = optimize_latents(latents, scheduler, num_inference_steps,\n","                              input_im, text_embeddings, guide_s,\n","                              style_s,True)\n","    im = show_img(latents)\n","    title = \"{}_in_style_{}\".format(text[0], style_im.split(\".\")[0])\n","    # title = \"{}\".format(text[0])\n","    show_and_save_image(plt, loader(im).unsqueeze(0), title, path)"],"metadata":{"id":"YIIHQFVvMCqe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def optimize_latents(latents, scheduler, num_inference_steps, style_img,\n","                     text_embeddings, guidance_scale, style_weight,use_StyleLoss=True):\n","\n","    scheduler.set_timesteps(num_inference_steps)\n","    if use_StyleLoss:\n","      model,style_losses = get_style_model_for_diffusion(style_img)\n","    for i,t in enumerate(tqdm(scheduler.timesteps)):\n","        latent_model_input = torch.cat([latents] * 2)\n","        latent_model_input = scheduler.scale_model_input(latent_model_input, timestep=t)\n","\n","        with torch.no_grad():\n","          noise_pred = unet(latent_model_input, t, encoder_hidden_states=text_embeddings).sample\n","        noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n","\n","        noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n","\n","        if use_StyleLoss:\n","          z = noise_pred.clone().detach()\n","          z.requires_grad = True\n","          clean_latent_estimate = scheduler.convert_model_output(z, t, latents)\n","          clean_img = sl_grad(clean_latent_estimate,model,style_losses)\n","          with torch.no_grad():\n","            grad = z.grad / torch.mean(torch.abs(z.grad))\n","          sig_t = scheduler.sigma_t[t]\n","          noise_pred -= style_weight * sig_t * grad\n","\n","        latents = scheduler.step(noise_pred, t, latents).prev_sample\n","    torch.cuda.empty_cache()\n","\n","    return latents"],"metadata":{"id":"KQgMxl3w-QI7"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"private_outputs":true,"gpuType":"T4","collapsed_sections":["pNOFpNM1Sovn","78KJ-iLZsaQt","v1XGfs4JToQF"]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}