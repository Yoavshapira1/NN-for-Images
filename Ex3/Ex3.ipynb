{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard","accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"yodZN_lKhwhE"},"outputs":[],"source":["# Imports\n","!pip install torch\n","!pip install torchvision\n","!pip install matplotlib\n","import numpy as np\n","from matplotlib import pyplot as plt\n","import matplotlib.animation as animation\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch\n","import torchvision\n","import torchvision.transforms as transforms\n","import torchvision.utils as vutils\n","from torch.utils.data import ConcatDataset, DataLoader\n","from functools import partial\n","import torch.optim as optim\n","device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""]},{"cell_type":"code","source":["# Connect to Google Drive\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","root_dir = '/content/gdrive/MyDrive/NN for Images/Ex3'"],"metadata":{"id":"4DRPQuE6h6Ui","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1684828486724,"user_tz":-180,"elapsed":20635,"user":{"displayName":"יואב שפירא Yoav Shapira","userId":"05185126381067139480"}},"outputId":"d31f6f28-fa6e-422d-dc97-47d05d2a4426"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}]},{"cell_type":"code","source":["# Reproducability\n","import random\n","manualSeed = 999\n","random.seed(manualSeed)\n","torch.manual_seed(manualSeed)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2dXa8zpAqGuN","executionInfo":{"status":"ok","timestamp":1684828486726,"user_tz":-180,"elapsed":14,"user":{"displayName":"יואב שפירא Yoav Shapira","userId":"05185126381067139480"}},"outputId":"35d51d9d-e0bc-4e3d-b64f-a0711d907d78"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x7f6ef054a1d0>"]},"metadata":{},"execution_count":3}]},{"cell_type":"markdown","source":["# **Data Loaders**\n","The dataset I used is the full MNIST, including both train and test sets"],"metadata":{"id":"k2xOb7J_i9JT"}},{"cell_type":"code","source":["def get_dataloader(batch_size=128):\n","    transform = transforms.ToTensor()\n","\n","    dataset_train = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n","    dataset_test = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n","    dataset_merged = ConcatDataset([dataset_train, dataset_test])\n","\n","    loader = torch.utils.data.DataLoader(dataset_merged, batch_size=batch_size, shuffle=True)\n","\n","    return loader"],"metadata":{"id":"-uay6y6ViwZZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Global Parameters & Helper Functions**"],"metadata":{"id":"K5y2Kd_35vLE"}},{"cell_type":"markdown","source":["### **Global Parameters**\n","Definitions of dimensions labels"],"metadata":{"id":"VzqW7EqYdC6L"}},{"cell_type":"code","source":["image_size = 28    # The 1-dimension of the image in the dataset\n","im_c = 1           # The number of channels per image in the dataset\n","z_size = 100       # The size latnent vector Z which is the input for G\n","batch_size = 128\n","dataloader = get_dataloader(batch_size)\n","\n","# labling the ground truth and the faked\n","real_label = 1.\n","fake_label = 0.\n","\n","# functions that create vector of 'real' / 'fake' labels for a given size\n","real_label_vec = lambda n : torch.full((n,), real_label, dtype=torch.float)\n","fake_label_vec = lambda n : torch.full((n,), fake_label, dtype=torch.float)"],"metadata":{"id":"hdkeCKJk5u4H"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Weights Initialization**\n","The original DCGAN paper mentions that the initialized weights of all modules, should be taken from a Normal Distribution with `mean=0`, and `std=0.2`. This function will apply this criteria to our modules."],"metadata":{"id":"KiKVzCM2s-Op"}},{"cell_type":"code","source":["def weights_init(m):\n","    classname = m.__class__.__name__\n","    if classname.find('Conv') != -1:\n","        nn.init.normal_(m.weight.data, 0.0, 0.02)\n","    elif classname.find('BatchNorm') != -1:\n","        nn.init.normal_(m.weight.data, 1.0, 0.02)\n","        nn.init.constant_(m.bias.data, 0)"],"metadata":{"id":"BxBab22NtBJ_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Animate List of Images**\n","This function creates an animation gif made of list of images.\n","\n","In our case the images are grid of 64 small images that G genreated, helping to monitor it's performance."],"metadata":{"id":"0JXJK39Ljy2a"}},{"cell_type":"code","source":["def animate(img_list, name, dim=(8,8), iter_D_per_G=1):\n","    fig = plt.figure(figsize=dim)\n","    plt.axis(\"off\")\n","\n","    def update(i):\n","        plt.imshow(np.transpose(img_list[i], (1, 2, 0)), animated=True)\n","\n","    ani = animation.FuncAnimation(fig, update, frames=len(img_list), interval=1000)\n","    ani.save(\"%s/Q1/%s_%d:1.gif\" % (root_dir, name, iter_D_per_G))"],"metadata":{"id":"yOgnxzWVj4bd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Generator Loader**\n","Load the proper Generator given a `Config` object"],"metadata":{"id":"mA00KgXilSRr"}},{"cell_type":"code","source":["# Generator loader\n","def load_G(con):\n","    path = r'{}/Q1/{}_with_loss_{}_{}:1.pth'\n","    G = Generator(con.Gfeatures)\n","    G.load_state_dict(torch.load(path.format(root_dir, \"G\", con.loss, con.iter_D_per_G)))\n","    G.eval()\n","    return G"],"metadata":{"id":"iXMnPaESFuqZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Images Plotter**\n","Helper function to plot Q2's and Q3's results\n","\n"],"metadata":{"id":"0r3rLYzQPmz2"}},{"cell_type":"code","source":["def show_images(im_list, path, rect_coor):\n","    slots = len(im_list)\n","    if slots == 2:\n","        original, reconstructed = im_list\n","    else:\n","        original, damaged, reconstructed = im_list\n","\n","    fig, axs = plt.subplots(1, slots, figsize=(8, 4))\n","\n","    # Plot the original  image\n","    axs[0].imshow(original.squeeze().detach().numpy(), cmap='gray')\n","    axs[0].axis('off')\n","    axs[0].set_title('Original Image')\n","\n","    # Plot the damaged image, in case of 3 images given\n","    if slots == 3:\n","        axs[1].imshow(damaged.squeeze().detach().numpy(), cmap='gray')\n","        axs[1].axis('off')\n","        axs[1].set_title('Damaged Image')\n","        if rect_coor[0] is not None:\n","          top, left = rect_coor\n","          rect = plt.Rectangle((left, top), 8, 8, edgecolor='red', linewidth=5, fill=False)\n","          axs[1].add_patch(rect)\n","\n","    # Plot the reconstructed image\n","    axs[slots-1].imshow(reconstructed.squeeze().detach().numpy(), cmap='gray')\n","    axs[slots-1].axis('off')\n","    axs[slots-1].set_title('Reconstructed Image')\n","\n","    plt.savefig(path)"],"metadata":{"id":"ypnYL8ouICUV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Configuration object**\n","To track the hyper parameters easily"],"metadata":{"id":"VgizHOSVO6xw"}},{"cell_type":"code","source":["# An object for tracking hyper parameters\n","class Config:\n","    def __init__(self, loss_fn, iter_D_per_G=3):\n","        self.Gfeatures = 64\n","        self.Dfeatures = 32\n","        self.iter_D_per_G = iter_D_per_G\n","        self.num_epochs = 10\n","        self.lr = 0.0002\n","        self.l2_reg = 0.5\n","        self.loss = loss_fn"],"metadata":{"id":"KD1ttrcD7cMH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Modules Definitions**"],"metadata":{"id":"nZlOTrxHdy8o"}},{"cell_type":"markdown","source":["### **Generator**\n","The Generator recieves a vector Z of size `z_size` (=100), which viewd here as a 1 x 1 image with 100 channels. It outputs an images of size `im_size x im_size` (28 x 28) with `im_c` (=1) channels. The channels amount in every Upconv layer is a factor of `Gfeatures` (=64). The exact architecture is as follow:\n","\n","1.   Transposed 2-Strided Convolution (kernel=3, in=100, out=256)\n","2.   Batch Normalization\n","3.ReLU\n","1.   Transposed 2-Strided Convolution (kernel=3, in=256, out=128)\n","2.   Batch Normalization\n","3.ReLU\n","1.   Transposed 2-Strided Convolution (kernel=3, in=128, out=64)\n","2.   Batch Normalization\n","3.ReLU\n","1.   Transposed 2-Strided Convolution (kernel=3, in=64, out=1)\n","2.   Sigmoid\n","\n","\n"],"metadata":{"id":"kN6nW2eMpFXU"}},{"cell_type":"code","source":["class Generator(nn.Module):\n","    def __init__(self, feat):\n","        super(Generator, self).__init__()\n","        self.seq = nn.Sequential(\n","            # Input size: 100 x 1 x 1\n","\n","            nn.ConvTranspose2d(in_channels=100, out_channels=4 * feat,\n","                               kernel_size=3, stride=2, padding=0, bias=False),\n","            nn.BatchNorm2d(4 * feat),\n","            nn.ReLU(inplace=True),\n","            # current size: 4*feat x 4 x 4\n","\n","            nn.ConvTranspose2d(in_channels=4 * feat, out_channels=2 * feat,\n","                               kernel_size=3, stride=2, padding=0, bias=False),\n","            nn.BatchNorm2d(2 * feat),\n","            nn.ReLU(inplace=True),\n","            # current size: 2*feat x 7 x 7\n","\n","            nn.ConvTranspose2d(in_channels=2 * feat, out_channels=feat,\n","                               kernel_size=3, stride=2, padding=0, bias=False),\n","            nn.BatchNorm2d(feat),\n","            nn.ReLU(inplace=True),\n","            # current size: feat x 16 x 16\n","\n","            nn.ConvTranspose2d(in_channels=feat, out_channels=1,\n","                               kernel_size=3, stride=2, padding=2, output_padding=1, bias=False),\n","            # current size: 1 x 28 x 28\n","\n","            # For the last layer, the activation is  Sigmoid\n","            nn.Sigmoid()\n","        )\n","\n","    def forward(self, z):\n","       return self.seq(z)"],"metadata":{"id":"EBf-x0ONpE5x"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Discriminator**\n","The Discriminator recieves an images of size `im_size x im_size` (28 x 28) with `im_c` (=1) channels, and outputs a single scalar value that represents the probability for this image to be real. The channels amount in every Upconv layer is a factor of `Dfeatures` (=32). The exact architecture is as follow:\n","\n","1.   2-Strided Convolution (kernel=4, in=1, out=32)\n","2.   Batch Normalization\n","3.LeakyReLU (slope=0.2)\n","1.   2-Strided Convolution (kernel=4, in=32, out=64)\n","2.   Batch Normalization\n","3.LeakyReLU (slope=0.2)\n","1.   2-Strided Convolution (kernel=4, in=64, out=128)\n","2.   Batch Normalization\n","3.LeakyReLU (slope=0.2)\n","5. Flatten\n","6. Fully-Connected (in=128, out=1)\n","2.   Sigmoid"],"metadata":{"id":"twm7J5-TX7UY"}},{"cell_type":"code","source":["class FlattenBatch(nn.Module):\n","    # This is a module that only flatten a given batch\n","    def __init__(self):\n","        super(FlattenBatch, self).__init__()\n","\n","    def forward(self, x):\n","        return x.flatten(1)\n","\n","class Discriminator(nn.Module):\n","    def __init__(self, feat):\n","        super(Discriminator, self).__init__()\n","        self.seq = nn.Sequential(\n","            # current size: 1 x 28 x 28\n","            nn.Conv2d(in_channels=1, out_channels=feat, kernel_size=4, stride=2, padding=1),\n","            nn.BatchNorm2d(feat),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            # current size: feat x 14 x 14\n","\n","            nn.Conv2d(in_channels=feat, out_channels=2 * feat, kernel_size=4, stride=2, padding=1),\n","            nn.BatchNorm2d(2 * feat),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            # current size: 2*feat x 7 x 7\n","\n","            nn.Conv2d(in_channels=2 * feat, out_channels=4 * feat, kernel_size=4, stride=2, padding=1),\n","            nn.BatchNorm2d(4 * feat),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            # current size: 4*feat x 4 x 4\n","\n","            # custom layer that flatten the batch\n","            FlattenBatch(),\n","            nn.Linear(4*feat * 3 * 3, 1),\n","\n","            nn.Sigmoid()\n","        )\n","\n","    def forward(self, x):\n","        return self.seq(x)"],"metadata":{"id":"vRRw1wqQX9mR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Loss functions**\n","Definitions of the 3 different loss functions we investigated:\n","\n","\n","*   **The Original DCGAN Loss:**\n","In this case, the loss of D is the `Binary-Cross-Entropy(BCELoss)` loss, G is trying to minimze `log(1-D(G(z)))`. In order to achieve that, G's loss will be defined as `BCELoss` that compare D's predicitions with real-labels only, and negate the results.\n","*   **The Non-Saturating Loss:**\n","In this case, G is trying to maximize `log(D(G(z)))`, or equivalently minimize `-log(D(G(z))).` To achieve that, the loss will be defined as `BCELoss` that compare D's predicitions with fake-labels only.\n","\n","\n","\n","*   **The L2 Loss:** This case is very similar to Non-Saturating case, but here D's loss is `MSELoss` that corresponding to L2.\n","\n","\n","\n"],"metadata":{"id":"GZrE_9aI90Yi"}},{"cell_type":"code","source":["def get_loss(loss_name):\n","  if loss_name not in ['Saturated', 'Non-Saturation', 'L2']:\n","    raise ValueError(\"loss_name should be one of: 'Saturated', 'Non-Saturation', 'L2'\")\n","\n","  if loss_name == 'Saturated':\n","    # The original loss; G minimizes probability of 'fake' responses of D\n","    D_loss = nn.BCELoss()\n","    G_loss = lambda x : - D_loss(x, fake_label_vec(x.size(0)))\n","\n","  elif loss_name == 'Non-Saturation':\n","    # The Non-Saturating loss; G maximizes the probability of 'real' responses of D\n","    D_loss = nn.BCELoss()\n","    G_loss = lambda x : D_loss(x, real_label_vec(x.size(0)))\n","\n","  elif loss_name == 'L2':\n","    # The L2 loss; G minimizes the MSE of 'real' responses\n","    D_loss = nn.MSELoss()\n","    G_loss = lambda x : D_loss(x, real_label_vec(x.size(0)))\n","\n","  return {\"D_loss\" : D_loss, \"G_loss\" : G_loss}"],"metadata":{"id":"66wJ8QqA94w1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Train Loop**"],"metadata":{"id":"PfKPmQw-t_B6"}},{"cell_type":"code","source":["def train_disciminator(D, G, data, criterion, optimizerD):\n","    D.zero_grad()\n","\n","    # Train with all-real batch\n","    output = D(data).view(-1)\n","    errD_real = criterion(output, real_label_vec(data.size(0)))\n","    errD_real.backward()\n","\n","    # Produces fake data\n","    noise = torch.randn(data.size(0), z_size, 1, 1)\n","    fake_data = G(noise)\n","\n","    # Train with all-fake batch\n","    output = D(fake_data.detach()).view(-1)\n","    errD_fake = criterion(output, fake_label_vec(data.size(0)))\n","    errD_fake.backward()\n","\n","    errD = errD_real + errD_fake\n","    optimizerD.step()\n","\n","    return errD.item(), fake_data"],"metadata":{"id":"zNWS6uCxRHsK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train_generator(D, G, fake_data, criterion, optimizerG):\n","    G.zero_grad()\n","\n","    # check out what D thinks this data is\n","    output = D(fake_data).view(-1)\n","\n","    # Update G accordingly to the loss function and the predictions\n","    errG = criterion(output)\n","    errG.backward()\n","    optimizerG.step()\n","\n","    return errG.item()"],"metadata":{"id":"M6jvAF_aRnqe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def trainloop(config=None, img_list=[], D_err=[], G_err=[]):\n","\n","    # batch of latent vectors (Z), that will help us to monitor the\n","    # performance of G along the training loop\n","    fixed_noise, iters = torch.randn(64, z_size, 1, 1), 0\n","\n","    # Initializes the Generator and the Discriminator and normalize the\n","    # weights to a Normal Distribution of mean=0, std=0.2\n","    G, D = Generator(config.Gfeatures), Discriminator(config.Dfeatures)\n","    G.apply(weights_init)\n","    D.apply(weights_init)\n","\n","    # Build the loss functions\n","    losses = get_loss(config.loss)\n","    D_loss, G_loss = losses[\"D_loss\"], losses[\"G_loss\"]\n","\n","    # Setup Adam optimizers for both G and D with L2 regularization\n","    optimizerD = optim.Adam(D.parameters(), lr=config.lr, betas=(config.l2_reg, 0.999))\n","    optimizerG = optim.Adam(G.parameters(), lr=config.lr, betas=(config.l2_reg, 0.999))\n","\n","    print(\"Starting Training Loop...\")\n","\n","    # For each epoch\n","    for epoch in range(config.num_epochs):\n","        for i, data in enumerate(dataloader):\n","\n","            # Train the Discriminator and produces fake data for the Generator training\n","            errD, fake_data = train_disciminator(D, G, data[0], D_loss, optimizerD)\n","\n","            # Train the Generator every <iter_D_per_G> batches, with the fake data\n","            if i % config.iter_D_per_G == 0:\n","                errG = train_generator(D, G, fake_data, G_loss, optimizerG)\n","\n","                # Plot the errors\n","                D_err.append(errD)\n","                G_err.append(errG)\n","\n","            # Print some statistics\n","            if i % 50 == 0:\n","                print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f' % (epoch, config.num_epochs, i, len(dataloader), errD, errG))\n","\n","            # Save G's output on some fixed noise vectors to monitor performance\n","            if (iters % 500 == 0) or ((epoch == config.num_epochs-1) and (i == len(dataloader)-1)):\n","                with torch.no_grad():\n","                    fake = G(fixed_noise).detach()\n","                    img_list.append(vutils.make_grid(fake, padding=2, normalize=True))\n","            iters += 1\n","\n","    # Svae the trained modules\n","    path = r'{}/Q1/{}_with_loss_{}_{}:1.pth'\n","    torch.save(G.state_dict(), path.format(root_dir, \"G\", config.loss, config.iter_D_per_G))\n","    torch.save(D.state_dict(), path.format(root_dir, \"D\", config.loss, config.iter_D_per_G))"],"metadata":{"id":"nIIOJ8KQuAY3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Q1**\n","Investigating the 3 different losses"],"metadata":{"id":"X0C2jQqstcz3"}},{"cell_type":"code","source":["def Q1():\n","\n","  # Try different loss functions\n","  for loss_fn in ['Saturated', 'Non-Saturation', 'L2']:\n","      con = Config(loss_fn=loss_fn)\n","      img_list, D_err, G_err = [], [], []\n","\n","      try:\n","        trainloop(con, img_list, D_err, G_err)\n","      except Exception as e:\n","        pass\n","\n","      # plot the loss and animate the progress even when fail\n","      finally:\n","        plt.cla()\n","        plt.title(loss_fn+\" loss\")\n","        x = [i for i in range(len(D_err))]\n","        plt.plot(x, D_err, alpha=0.5, label=\"D loss\")\n","        plt.plot(x, G_err, alpha=0.5, label=\"G loss\")\n","        plt.legend()\n","        plt.savefig('%s/Q1/%s_loss__%d:1.png'%(root_dir, loss_fn, con.iter_D_per_G))\n","        animate(img_list, loss_fn, dim=(8,8))\n","        print(\"Figures saved successfully\")"],"metadata":{"id":"BrkqC8D9tkqo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Q2**\n","Find optimal Z for a given image"],"metadata":{"id":"OYsXOvbUPakP"}},{"cell_type":"code","source":["def find_optimal_latent_vec(im, G, loss=\"L2\", steps=1000, lr=0.01):\n","\n","    # Create a latent vector, just noise\n","    z = torch.randn(1, z_size, 1, 1)\n","    z.requires_grad = True\n","\n","    # Choose loss function accordingly to 'loss'\n","    criterion = nn.MSELoss() if loss == \"L2\" else nn.L1Loss()\n","\n","    # The optimizer performs only on the latent vector z\n","    optimizer = optim.Adam([z], lr)\n","\n","    # Perform optimization\n","    for step in range(steps):\n","        optimizer.zero_grad()\n","\n","        # Generate image from the latent vector\n","        generated_image = G(z)\n","\n","        # Compute the loss between the generated image and the target image\n","        loss = criterion(generated_image, im)\n","\n","        # Backpropagate the gradients\n","        loss.backward()\n","        optimizer.step()\n","\n","        # # Uncomment here to print the loss every 100 steps\n","        # if step % 100 == 0:\n","        #     print(\"Step [{}/{}], Loss: {:.4f}\".format(step, steps, loss.item()))\n","\n","    # Return the optimized latent vector\n","    return z"],"metadata":{"id":"qcEV7HONDXl6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def Q2(loss_fn, iter_D_per_G):\n","\n","    # Load the proper G in an evaluation mode the prevent updating\n","    con = Config(loss_fn=loss_fn, iter_D_per_G=iter_D_per_G)\n","    G = load_G(con)\n","\n","    # Create dataloader for single images\n","    single_dataloader = get_dataloader(batch_size=1)\n","\n","    # Use GAN inversion for 5 different images\n","    for i in range(5):\n","\n","        # Grab a random image from the MNIST dataset\n","        image, _ = next(iter(single_dataloader))\n","\n","        # Find an optimal latent vector that reconstruct this image\n","        optimal_Z = find_optimal_latent_vec(image, G, \"L2\")\n","\n","        # Generate the reconstructed image from the inverted latent vector\n","        reconstructed_image = G(optimal_Z)\n","\n","        # Display the target image and the reconstructed image\n","        path = '%s/Q2/%s_loss__%d:1_%d.png'%(root_dir, con.loss, con.iter_D_per_G, i)\n","        show_images([image, reconstructed_image], path)"],"metadata":{"id":"kyud4nLFtHd9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Q3**\n","Reconstructing corrupted images using GAN inversion"],"metadata":{"id":"XnaATFUeQB2c"}},{"cell_type":"code","source":["# Adding noise taken from Normal distribution with mean=0 and std=0.1\n","def add_gaussian_noise(im):\n","    noise = torch.randn_like(im) * 0.1\n","    noisy_image = im + noise\n","    return noisy_image, None, None"],"metadata":{"id":"2yyNof2zRS-N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def paint_random_window_black(image, window_size=8):\n","\n","    # Choose coordinate randomly\n","    top = random.randint(5, image_size - window_size - 5)\n","    left = random.randint(5, image_size - window_size - 5)\n","\n","    # paint the window in black\n","    clone = image.clone()\n","    clone[0][0][top:top+8, left:left+8] = 0\n","\n","    return clone, top, left"],"metadata":{"id":"5VDe_2CBUNmO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def Q3(loss_fn, iter_D_per_G):\n","\n","    # Load the proper G in an evaluation mode the prevent updating\n","    con = Config(loss_fn=loss_fn, iter_D_per_G=iter_D_per_G)\n","    G = load_G(con)\n","\n","    # Create dataloader for single images\n","    single_dataloader = get_dataloader(batch_size=1)\n","\n","    # Definitions: For denoising choose MSE, for inpainting choose L1\n","    problems = [\"Denoising\", \"Inpainting\"]\n","    damaging_func = [add_gaussian_noise, paint_random_window_black]\n","    loss = [\"L2\", \"L1\"]\n","\n","    # Use GAN inversion on 10 images of each method: Denoising & Inpainting\n","    for method, foo, l in zip(problems, damaging_func, loss):\n","        for i in range(30):\n","            if method == \"Denoising\":\n","              break\n","\n","            # Grab a random image from the MNIST dataset\n","            orig_image, _ = next(iter(single_dataloader))\n","\n","            # Corrupt the image. In case of inpainting, the top&left are the window's corner\n","            corrupt_image, top, left = foo(orig_image)\n","\n","            # Find an optimal latent vector that reconstruct this image\n","            optimal_Z = find_optimal_latent_vec(corrupt_image, G, l, steps=1000, lr=0.01)\n","\n","            # Generate the reconstructed image from the inverted latent vector\n","            reconstructed_image = G(optimal_Z)\n","\n","            # Display the target image and the reconstructed image\n","            path = '%s/Q3/%s/%s_loss__%d:1_%d.png'%(root_dir, method, con.loss, con.iter_D_per_G, i)\n","            show_images([orig_image, corrupt_image, reconstructed_image], path, rect_coor=[top,left])\n",""],"metadata":{"id":"tb7lmROOQDq9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Prompt Runners**"],"metadata":{"id":"eeqcE99xlXh-"}},{"cell_type":"code","source":["Q1()"],"metadata":{"id":"AFNsrvfeLH-G"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["Q2('Non-Saturation', 3)"],"metadata":{"id":"4spz1xqrIsKY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["Q3('Non-Saturation', 3)"],"metadata":{"id":"btuyavFYJCYe"},"execution_count":null,"outputs":[]}]}