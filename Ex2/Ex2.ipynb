{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1rTHLe50DvCyFKl1CfisKTX9h3hwomfsq","timestamp":1681748255167}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["# **imports**"],"metadata":{"id":"UD7rqnEVazcm"}},{"cell_type":"code","source":["!pip install wandb -qU\n","import wandb\n","from functools import partial\n","wandb.login(key='62bc107823f2e5d51fd5f21b1081d81dd76a07db')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oNDwPl3kLtpy","executionInfo":{"status":"ok","timestamp":1693562407191,"user_tz":-180,"elapsed":19894,"user":{"displayName":"Yoav Shapira","userId":"15591983862423815746"}},"outputId":"d6722c51-1615-401f-c30a-d780eae2740f"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.6/188.6 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m218.8/218.8 kB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":1}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Oi9SHJlhaqzF"},"outputs":[],"source":["!pip install wandb -qU\n","!pip install torch\n","!pip install torchvision\n","!pip install matplotlib\n","import numpy as np\n","from matplotlib import pyplot as plt\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch\n","import torchvision\n","import torchvision.transforms as transforms\n","from functools import partial\n","import torch.optim as optim\n","\n","torch.manual_seed(0)\n","\n","device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/gdrive')\n","root_dir = '/content/gdrive/MyDrive/NN for Images/Ex2'"],"metadata":{"id":"8ZxCn7oVAHIj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1681828973335,"user_tz":-180,"elapsed":2214,"user":{"displayName":"Yoav Shapira","userId":"15591983862423815746"}},"outputId":"bec1d923-2c38-4bb5-b49c-8004ea82460f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"]}]},{"cell_type":"markdown","source":["# **Data Loaders**\n","The values `(0.1307,), (0.3081,)` are the mean and the standard deviation of the MNIST dataset."],"metadata":{"id":"9LNPy6aUdROQ"}},{"cell_type":"code","source":["def get_dataloader(is_train, batch_size=64):\n","    transform = transforms.Compose(\n","    [transforms.ToTensor(),\n","     transforms.Normalize((0.1307,), (0.3081,))])\n","\n","    dataset = torchvision.datasets.MNIST(root='./data', train=is_train,\n","                                            download=True, transform=transform)\n","    loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n","                                              shuffle=is_train)\n","\n","    return loader"],"metadata":{"id":"FMCxp1addTxo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Encoder Definition**\n","The Encoder here is a subject to the next parameters:\n","1. Amount of convolutional layers (`n_conv_layers`)\n","2. Kernel size, which is to be constant in all convolutional layers (`kernel_size`)\n","3. Increasing channels amount factor, which determines how many channels will be added to the next convolution layer (`c_factor`)\n","4. The latenet dimension, mentioned in the excercise as *d* (`latent_dim`)\n","\n","The architecture of the Encoder is basically:\n","\n","1.   ***Strided Convolution Layer***\n","2.   ***ReLU***\n","3.   *repeate 1-2 `n_conv_layers` times*\n","4.   ***Flatten***\n","4.   ***Fully Connected layer*** with output dimension of `1/3` of the last convolution output size\n","6.   **ReLU**\n","5.   ***Fully Connected layer*** with output dimension of `d`\n","\n","The Encoder object will be determined in a single-valued way with those, to mirror this specific architecture."],"metadata":{"id":"BTftYJQcceNf"}},{"cell_type":"code","source":["class Encoder(nn.Module):\n","    def __init__(self, n_conv_layers, kernel_size, c_factor, latent_dim, stride=2, padding=1):\n","        super().__init__()\n","        self.layers = nn.ModuleList()\n","        self.build_layers(n_conv_layers, c_factor, kernel_size, latent_dim, stride, padding)\n","\n","    def build_layers(self, n_conv_layers, cout, kernel_size, latent_dim, stride, padding):\n","        c_in = 1         # c_in is 1 for grayscale input\n","        cur_dim = 28     # size of input data\n","        c_factor = cout\n","\n","        self.dimensions = [cur_dim]\n","        # Build the convolution layers\n","        for l in range(n_conv_layers):\n","          self.layers.append(nn.Conv2d(c_in, cout, kernel_size, stride=stride, padding=padding))\n","          c_in = cout\n","          cur_dim = ((cur_dim + (2 * padding) - (kernel_size - 1) - 1) // stride) + 1\n","          self.dimensions.append(cur_dim)\n","          cout += c_factor\n","\n","        # Build the 2 fully-connected layers\n","        cur_dim = (cout-c_factor)*(self.dimensions[-1]**2)\n","        self.dimensions.append(cur_dim)\n","        self.layers.append(nn.Linear(cur_dim, cur_dim // 3))\n","        cur_dim = cur_dim // 3\n","        self.dimensions.append(cur_dim)\n","        self.layers.append(nn.Linear(cur_dim, latent_dim))\n","\n","    def forward(self, x):\n","        # Pass throught all the convolution layers\n","        for conv in self.layers[:-2]:\n","            x = F.relu(conv(x))\n","        x = torch.flatten(x, 1)\n","\n","        # Pass throught ths FC layers\n","        x = F.relu(self.layers[-2](x))\n","        x = self.layers[-1](x)\n","        return x\n","\n","    def get_decoder_architecture(self):\n","        # This function returns an information about the exact architecture\n","        # of this Encoder instance, to help building the Encoder architecture\n","        arch = []\n","        for layer in self.layers:\n","            for p in layer.parameters():\n","                arch.append(p.shape)\n","                break\n","        return arch[::-1], self.dimensions, 2"],"metadata":{"id":"rP2h5qjbci1u"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Decoder Definition**\n","Since the decoder is mirroring the encoder, it determined exactly (in a single-valued way) by the architecutre of the encoder. Hence, the parameters for this model are subject to the Encoder and are given by the Encoder's method `get_decoder_architecture()`.\n","\n","The architecture of the Decoder is basically:\n","\n","1.   ***Fully Connected layer***\n","2.   ***ReLU***\n","3.   ***Fully Connected layer***\n","4.   ***ReLU***\n","5.   ***Reshape***  \n","1.   ***Convolution 2D Transpose Layer***\n","2.   ***ReLU***\n","3.   *repeate 6-7 `n_conv_layers-1` times*\n","4.   ***Convolution 2D Transpose Layer***\n","10.  ***Sigmoid***\n","\n","Notice that the after the *Conv2DTranspose* last layer (or the singlel layer if there is only one), There is no ReLU activation since the *Sigmoid* activates as an activation layer.\n"],"metadata":{"id":"6ExmPoC_ci8H"}},{"cell_type":"code","source":["class Decoder(nn.Module):\n","    # The architecture argument holds the dimensions of all the layers\n","    # in the Encoder object, from the last to the first\n","    # The num_of_fc is in fact 2, and here only for making my coding routine\n","    # easier and make the Decoder depends exlusively on the Encoder\n","\n","    def __init__(self, architecture, encoder_dimensions, num_of_fc, stride=2, padding=1):\n","        super().__init__()\n","        self.layers = nn.ModuleList()\n","\n","        # The dimension of the Encoder's FC layer is important for correctly\n","        # reshaping the data after the FC layer here.\n","        # The shape is (Cin, LastD, LastD), where:\n","        # Cin = The number of channels of Encoder's last convolution layer\n","        # LastD = The spatial dimensions of the data when processing by the\n","        #         Encoder's last convolution layer\n","        self.num_of_fc = num_of_fc\n","        self.reshape_shape = (-1, architecture[num_of_fc][0], encoder_dimensions[-num_of_fc-1], encoder_dimensions[-num_of_fc-1])\n","        self.build_layers(architecture, encoder_dimensions[:-num_of_fc-1], num_of_fc, stride, padding)\n","\n","    def build_layers(self, architecture, encoder_dimensions, num_of_fc, stride, padding):\n","\n","        # The first elements in architecture is encoder's FC layers dimensions\n","        for i in range(num_of_fc):\n","          self.layers.append(nn.Linear(architecture[i][0], architecture[i][1]))\n","\n","        # All others are convolution layers dimensions\n","        # If stirde > 1, the natural ConvTranspose2d will result in an odd\n","        # dimension, hence for an even target dimension, an \"outer_padding\"\n","        # is required\n","        for layer, dim in zip(architecture[num_of_fc:], encoder_dimensions[::-1]):\n","            if dim % 2 == 0 and stride > 1:\n","                self.layers.append(nn.ConvTranspose2d(layer[0], layer[1], layer[2], stride=stride, padding=padding,\n","                                                      output_padding=1))\n","            else:\n","                self.layers.append(nn.ConvTranspose2d(layer[0], layer[1], layer[2], stride=stride, padding=padding))\n","\n","    def forward(self, x):\n","        # Pass throught the FC layers\n","        for i in range(self.num_of_fc):\n","            x = F.relu(self.layers[i](x))\n","        x = torch.reshape(x, self.reshape_shape)\n","\n","        # Pass throught all the upconv layers\n","        for upconv in self.layers[self.num_of_fc:-1]:\n","            x = F.relu(upconv(x))\n","        x = self.layers[-1](x)\n","        return F.sigmoid(x)"],"metadata":{"id":"33X0ReC5cy5o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_auto_encoder(param):\n","    # parameters format:\n","    # [n of conv layers, kernel size, increasing channels factor, latent dim,\n","    # stride, padding]\n","\n","    encoder = Encoder(*param).to(device)\n","    decoder_architecture, encoder_dimensions, num_of_fc = encoder.get_decoder_architecture()\n","    decoder = Decoder(decoder_architecture, encoder_dimensions, num_of_fc, param[4], param[5]).to(device)\n","\n","    return nn.Sequential(encoder, decoder).to(device)"],"metadata":{"id":"2Wi-Yexeo0Rf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Evaluation**\n","In this exercise, the evaluation is only for checking out the test loss, which is either\n","*   Reconstruction error: (`|AE(I) - I|`), corresponding to the MSE loss since power(2) is monotonic and sinlge-valued mapping\n","* Cross Entropy Loss: For classification tasks"],"metadata":{"id":"bjZc8jQfhhr-"}},{"cell_type":"code","source":["def evaluate(model, test_set_loader, criterion, classification=False):\n","    test_loss = []\n","\n","    # Evaluation shouldn't change the gradients calculation\n","    with torch.no_grad():\n","        for data in test_set_loader:\n","            inputs, labels = data[0].to(device), data[1].to(device)\n","            outputs = model(inputs)\n","\n","            if classification:\n","                # For classification, the criterin is CrossEntropyLoss\n","                # operates on predictions and ground-truth labels\n","                loss = criterion(outputs, labels)\n","\n","            else:\n","                # For reconstruction, the criterion is MSELoss, operates\n","                # only on inputs and predictions\n","                loss = criterion(outputs, inputs)\n","\n","            test_loss.append(loss.detach().cpu().numpy())\n","    test_loss = np.mean(test_loss)\n","    test_metrics = {\"Average test loss\": test_loss}\n","    wandb.log(test_metrics)"],"metadata":{"id":"oY4Gx89fh48O"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Train Loop for AutoEncoder**\n","**This is the train loop for the AutoEncoders training, and not for the classification tasks.**\n","\n","`model` here is a `nn.Sequential` object contains the Encoder and the Decoder by the order, and used here as a single model.\n","The criterion is MSE loss, which is correspondings to the reconstruction error `|AE(I) - I|`."],"metadata":{"id":"Hj3AdaRAGDUC"}},{"cell_type":"code","source":["def train_loop(config=None, dir_path=None):\n","    with wandb.init(config=config):\n","        config = wandb.config\n","\n","        # Create data loaders iterators\n","        train_data_loader = get_dataloader(is_train=True, batch_size=config.batch_size)\n","        test_data_loader = get_dataloader(is_train=False, batch_size=config.batch_size)\n","\n","        # Define the model and the parameters\n","        param = [config.num_of_conv, config.kernel, config.c_factor,\n","                 config.latent_dim, config.stride, config.padding]\n","        AE = get_auto_encoder(param)\n","        optimizer = optim.AdamW(AE.parameters(), lr=config.lr)\n","        criterion = nn.MSELoss()\n","\n","        # Train the model and log the averaged losses\n","        for epoch in range(config.epochs):\n","            epoch_loss = []\n","            for data in train_data_loader:\n","                inputs = data[0].to(device)\n","\n","                # Update model's parameters with gradient descend method\n","                outputs = AE(inputs)\n","\n","                batch_loss = criterion(outputs, inputs)\n","                optimizer.zero_grad()\n","                batch_loss.backward()\n","                optimizer.step()\n","\n","                epoch_loss.append(batch_loss.detach().cpu().numpy())\n","\n","            # Plot the loss\n","            metrics = {\"Average train loss\": np.mean(epoch_loss)}\n","            wandb.log(metrics)\n","\n","            # Evaluate the model's phase and plot it\n","            evaluate(AE, test_data_loader, criterion)\n","\n","        if dir_path is not None:\n","          torch.save(AE.state_dict(), dir_path.format(config.num_of_conv, config.latent_dim))"],"metadata":{"id":"6Hgk6OAdGFEX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Q1: Explore the reconstruction error**\n","1. Using lower and higher latent space dimension *d*\n","2. using a fixed latent dimension *d* but with encoder/decoder architecture with more or fewer layers/weights.\n","\n","I have implemented a 'sweep' report for searching the best parameters. The method 'grid' is looking for the best combination of values from a given values lists. The lists are called `sweep_configuration`.\n","\n","In the W&B dashboard I then have isolated *d*, and the size of the AE and the results are reported in the PDF."],"metadata":{"id":"VMppGP1giAxB"}},{"cell_type":"code","source":["def Q1():\n","    spec_sweep_configuration = {\n","        'method': 'grid',\n","        'name': 'Ex2_Q1_AdamW',\n","        'metric': {\n","            'goal': 'minimize',\n","            'name': 'Average test loss'\n","            },\n","        'parameters': {\n","            \"num_of_conv\": {'values': [1 ,2 ,3 ,4, 5]},\n","            \"c_factor\": {'value': 8},\n","            \"latent_dim\": {'values': [5, 10, 15, 20]},\n","            \"kernel\": {'value': 3},\n","            \"stride\": {'value': 2},\n","            \"padding\": {'value': 1},\n","            \"epochs\": {'value': 15},\n","            \"lr\": {'value': 0.001},\n","            \"batch_size\": {'value': 16}\n","        }\n","    }\n","\n","\n","    my_sweep = wandb.sweep(spec_sweep_configuration, project=\"Ex2_2_FC\")\n","    train = partial(train_loop, spec_sweep_configuration)\n","    wandb.agent(my_sweep, function=train)"],"metadata":{"id":"ikmz3B5liTSn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Q2: Interpolation**\n","Here I've run forparameters:\n","`d = {5, 10, 15, 20}`,\n","`num_of_conv = 3`,\n","`kernel = 3`,\n","`c_factor= 8`,\n","Then I chose the `d = {10, 20}` for report results.\n","\n","I have calculated the interpolation for 20 different values of alpha, to create a continious interpolation plots.\n","\n"],"metadata":{"id":"pAm4zjv1_HjI"}},{"cell_type":"code","source":["def get_digits():\n","  # Extract two images of a random pair of different digits\n","\n","  pair = torch.randint(0, 9, (2,))\n","  while pair[0].item() == pair[1].item():\n","      pair = torch.randint(0, 9, (2,))\n","  digit_x, digit_y = pair[0].item(), pair[1].item()\n","\n","  for batch in get_dataloader(True):\n","      imgs, labels = batch[0], batch[1]\n","      images_x, images_y = imgs[labels == digit_x], imgs[labels == digit_y]\n","      img_x = images_x[torch.randint(0, images_x.shape[0], (1,))]\n","      img_y = images_y[torch.randint(0, images_y.shape[0], (1,))]\n","      return [img_x, img_y], [digit_x, digit_y]"],"metadata":{"id":"DnWM8mFDIZnM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def interpolate(model, dim, pair, digits):\n","    # Extract the Encoder and the Decoder as separate models\n","    encoder, decoder = model[0], model[1]\n","\n","    # Preparing the ground for the plot, which is to be a concatenated images\n","    result = np.zeros((28, 1))\n","    for alpha in np.linspace(0, 1, 20):\n","        first = encoder(pair[0]) * alpha\n","        second = encoder(pair[1]) * (1 - alpha)\n","        dec = decoder(first + second).detach().numpy()\n","        result = np.hstack((result, dec.reshape(28, 28)))\n","\n","    # Plot the data\n","    plt.cla()\n","    plt.suptitle(\"digits: {}\".format(digits))\n","    plt.imshow(result, cmap='gray')\n","    plt.xticks([])\n","    plt.yticks([])\n","    plt.savefig(root_dir + r'/Interpolations/digits_{}_and_{}_d_{}.png'.format(digits[0], digits[1], dim))"],"metadata":{"id":"aPf29eKQIZps"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def Q2(train_and_save_models=False, latent_dim=None, load=False, pairs=5):\n","    # determine configuration\n","    q2_sweep_parameters = {\n","        'method': 'grid',\n","        'name': 'Training_For_Q2_ker_3',\n","        'metric': {\n","            'goal': 'minimize',\n","            'name': 'Average test loss'\n","            },\n","        'parameters': {\n","            \"num_of_conv\": {'value': 3},\n","            \"c_factor\": {'value': 8},\n","            \"latent_dim\": {'values': latent_dim},\n","            \"kernel\": {'value': 3},\n","            \"stride\": {'value': 2},\n","            \"padding\": {'value': 1},\n","            \"epochs\": {'value': 15},\n","            \"lr\": {'value': 0.001},\n","            \"batch_size\": {'value': 16}\n","        }\n","    }\n","\n","    # train the models and save them\n","    dir_path = root_dir + r'/models/model_conv_{}_d_{}.pth'\n","    if train_and_save_models:\n","        my_sweep = wandb.sweep(q2_sweep_parameters, project=\"Ex2_2_FC\")\n","        train = partial(train_loop, q2_sweep_parameters, dir_path)\n","        wandb.agent(my_sweep, function=train)\n","\n","    if load:\n","      device = 'cpu'\n","      # Interpolate two images of random pair of digits\n","      for i in range(pairs):\n","          pair, digits = get_digits()\n","          for dim in latent_dim:\n","              model = get_auto_encoder([3, 3, 8, dim, 2, 1])\n","              model.load_state_dict(torch.load(dir_path.format(3, dim)))\n","              model.eval()\n","              interpolate(model, dim, pair, digits)"],"metadata":{"id":"3zNvFgkd_Gle"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Q3: Correlations**\n","The correlation between every two featurs in the latent dimension is given by Pearson's Correlation Matrix. Here the matrix is calculated using 8000 random images (note that the dataloader is shuffling the data in the proper function).\n","\n","The final measure to estimate the total correlations that I chose is the mean of the absolute values in the matrix, since also negative values are indicators for existing correlation."],"metadata":{"id":"lAx3cSW0dZY_"}},{"cell_type":"code","source":["def calc_corr(AE):\n","    # The final measure I chose is the mean of the absolute values in the\n","    # Pearson's correlation matrix\n","    encoder = AE[0]\n","    dataloader = get_dataloader(is_train=True, batch_size=8000)\n","    for data in dataloader:\n","        latent_vector = encoder(data[0]).detach().numpy()\n","        corr_mat = np.corrcoef(latent_vector.T)\n","        return np.mean(np.abs(corr_mat))"],"metadata":{"id":"6IlMsuacC1_6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def Q3(dimensions, path_for_model):\n","    device = 'cpu'\n","    abs_val_means = []\n","    for dim in dimensions:\n","        AE = get_auto_encoder([3, 3, 8, dim, 2, 1])\n","        AE.load_state_dict(torch.load(path.format(dim), map_location=torch.device('cpu')))\n","        AE.eval()\n","        mean_abs = Q3(AE)\n","        abs_val_means.append(mean_abs)\n","\n","    plt.plot(dimensions, abs_val_means)\n","    plt.title(\"Mean of Abs. values\")\n","    plt.xticks(dimensions)\n","    plt.yticks(abs_val_means)\n","    plt.xlabel(\"d\")\n","    plt.show()"],"metadata":{"id":"7kiNdjr_Cc7c"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Q4: Trasnfer Learning**\n","In this section we investigate the performence of classification task, based on the AutoEncoders we have trained before, in two different ways:\n","\n","\n","1.   Train only the MLP\n","2.   Train both the MLP and the pre-trained AutoEncoder\n","\n","The MLP architecture is:\n","\n","\n","*   ***Fully Connected*** with output dimension given to the constructor\n","*   ***ReLU***\n","*   ***Fully Connected*** with output dimension of 100\n","*   ***ReLU***\n","*   ***Fully Connected*** with output dimension of 10\n","\n","\n","\n"],"metadata":{"id":"URb3kfIJMNjb"}},{"cell_type":"code","source":["class MLP(nn.Module):\n","    # A simple multi layered perceptron including 3 fully-connected layers,\n","    # RelU activations, and sigmoid with argMax extraction for classifying\n","\n","    def __init__(self, input_dim, hidden_dim):\n","        super().__init__()\n","        self.fc1 = nn.Linear(input_dim, hidden_dim)\n","        self.fc2 = nn.Linear(hidden_dim, 100)\n","        self.fc3 = nn.Linear(100, 10)\n","\n","    def forward(self, x):\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        x = self.fc3(x)\n","        return x"],"metadata":{"id":"aiAnDQDISdlo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Train Loop for MLP**\n","**This is a train loop for the classification task, either for the MLP alone or with the Encoder parameters.**\n","\n","There are several implementation differences that motivated me write another function for this, and this is only for convenience."],"metadata":{"id":"ALChH7mt3fyG"}},{"cell_type":"code","source":["def train_loop_Q4(config=None, path_for_model=None):\n","    with wandb.init(config=config):\n","        config = wandb.config\n","\n","        # Create data loaders iterators\n","        train_data_loader = get_dataloader(is_train=True, batch_size=config.batch_size)\n","        test_data_loader = get_dataloader(is_train=False, batch_size=config.batch_size)\n","\n","        # load the pre-trained AE to extract the encoder from\n","        # changing the device temporarily for this task\n","        device = 'cpu'\n","        AE = get_auto_encoder([3, 3, 8, config.latent_dim, 2, 1])\n","        AE = AE.to(device)\n","        AE.load_state_dict(torch.load(path_for_model.format(config.latent_dim), map_location=torch.device(device)))\n","\n","        # trasnfer the model to cuda, extract the encoder and build a MLP\n","        device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n","        encoder = AE[0].to(device)\n","        mlp = MLP(input_dim=config.latent_dim, hidden_dim=config.mlp_hidden_layer).to(device)\n","\n","        # define the proper parameters for optimizer according to the task\n","        if config.train_encoder:\n","            params = list(encoder.parameters()) + list(mlp.parameters())\n","        else:\n","            params = mlp.parameters()\n","        optimizer = optim.AdamW(params, lr=config.lr)\n","\n","        # For classification tasks, CrossEntropyLoss is a good choice\n","        criterion = nn.CrossEntropyLoss()\n","\n","        # Train the model and log the averaged losses\n","        for epoch in range(config.epochs):\n","            epoch_loss = []\n","            for data in train_data_loader:\n","                inputs, labels = data[0].to(device), data[1].to(device)\n","\n","                # As we required, train only on ~tens of images\n","                if len(epoch_loss) > 100:\n","                    break\n","\n","                # Update model's parameters with gradient descend method\n","                outputs = mlp(encoder(inputs))\n","                batch_loss = criterion(outputs, labels)\n","                optimizer.zero_grad()\n","                batch_loss.backward()\n","                optimizer.step()\n","\n","                epoch_loss.append(batch_loss.detach().cpu().numpy())\n","\n","            # Plot the loss\n","            metrics = {\"Average train loss\": np.mean(epoch_loss)}\n","            wandb.log(metrics)\n","\n","            # Evaluate the model's phase and plot it\n","            evaluate(nn.Sequential(encoder, mlp), test_data_loader, criterion, classification=True)"],"metadata":{"id":"HQthRfxiRaI_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def Q4(dimensions, path_for_model):\n","    # determine configuration\n","    q4_sweep_parameters = {\n","        'method': 'grid',\n","        'name': 'Q4',\n","        'metric': {\n","            'goal': 'minimize',\n","            'name': 'Average test loss'\n","            },\n","        'parameters': {\n","            \"num_of_conv\": {'value': 3},\n","            \"c_factor\": {'value': 8},\n","            \"latent_dim\": {'values': dimensions},\n","            \"kernel\": {'value': 3},\n","            \"stride\": {'value': 2},\n","            \"padding\": {'value': 1},\n","            \"epochs\": {'value': 15},\n","            \"lr\": {'value': 0.001},\n","            \"batch_size\": {'value': 16},\n","            \"train_encoder\": {'values': [True, False]},\n","            \"mlp_hidden_layer\": {'values': [200, 650, 1000]}\n","        }\n","    }\n","\n","    my_sweep = wandb.sweep(q4_sweep_parameters, project=\"Ex2_2_Q4\")\n","    train = partial(train_loop_Q4, q4_sweep_parameters, path_for_model)\n","    wandb.agent(my_sweep, function=train)"],"metadata":{"id":"zl6qBLjTMRIZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Just a Runner\n","Running the separate questions.\n","Making sure that before every question, the device is allocated to `'cuda'` if possible.\n","If loading models is required, the device is to be changed to `'cpu'` within the different questions."],"metadata":{"id":"tkbVrfcw_N-d"}},{"cell_type":"code","source":["device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n","Q1()\n","\n","device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n","Q2(train_and_save_models=False, latent_dim=[20, 15, 10, 5], load=True)\n","\n","device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n","Q3(dimensions=[20, 15, 10, 5], path_for_model=root_dir + r'/models/model_conv_3_d_{}.pth')\n","\n","device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n","Q4(dimensions=[20, 15, 10, 5], path_for_model=root_dir + r'/models/model_conv_3_d_{}.pth')"],"metadata":{"id":"xQPVCOru_ODD"},"execution_count":null,"outputs":[]}]}